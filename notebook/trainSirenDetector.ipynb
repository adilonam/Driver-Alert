{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Written by Gabriel Sarch\n",
    "# gabrielsarch@gmail.com\n",
    "#\n",
    "# Last edited: 4/20/2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file trains the Convolutional Neural Network\n",
    "## Uses siren, car, and background noises (split into 3-second chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RATE = 22050 #Hz - sample rate of training files\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# paths to numpy array files\n",
    "# These files contain numpy arrays in format (nSoundBytes x nSamples) \n",
    "# nSamples is usually = RATE*seconds\n",
    "# see generateTrainingData\n",
    "sirenFile = r'C:\\Users\\Gabe\\Documents\\SeniorDesign\\Data\\Senior Design data training\\sirenNoise.npy'\n",
    "backFile = r'C:\\Users\\Gabe\\Documents\\SeniorDesign\\Data\\Senior Design data training\\backNoise.npy'\n",
    "carFile = r'C:\\Users\\Gabe\\Documents\\SeniorDesign\\Data\\Senior Design data training\\carNoise.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array shapes before:\n",
      "(1839, 66150)\n",
      "(6566, 66150)\n",
      "(613, 66150)\n",
      " \n",
      "Array shape after (valid, train):\n",
      "(184, 66150)\n",
      "(1655, 66150)\n",
      "(657, 66150)\n",
      "(5909, 66150)\n",
      "(61, 66150)\n",
      "(552, 66150)\n"
     ]
    }
   ],
   "source": [
    "# Load in numpy arrays containing data & splits data up into training and validation sets (use validation for ROC curves)\n",
    "\n",
    "sirenNoise = np.load(sirenFile)\n",
    "backNoise = np.load(backFile)\n",
    "carNoise = np.load(carFile)\n",
    "\n",
    "print('Array shapes before:')\n",
    "print(sirenNoise.shape)\n",
    "print(backNoise.shape)\n",
    "print(carNoise.shape)\n",
    "print(' ')\n",
    "\n",
    "percentHeldOut = 0.1\n",
    "\n",
    "perm = np.random.permutation(len(sirenNoise))[0:round(percentHeldOut*sirenNoise.shape[0])]\n",
    "sirenNoiseVal = sirenNoise[perm]\n",
    "sirenNoise = np.delete(sirenNoise, perm, axis=0)\n",
    "\n",
    "perm = np.random.permutation(len(backNoise))[0:round(percentHeldOut*backNoise.shape[0])]\n",
    "backNoiseVal = backNoise[perm]\n",
    "backNoise = np.delete(backNoise, perm, axis=0)\n",
    "\n",
    "perm = np.random.permutation(len(carNoise))[0:round(percentHeldOut*carNoise.shape[0])]\n",
    "carNoiseVal = carNoise[perm]\n",
    "carNoise = np.delete(carNoise, perm, axis=0)\n",
    "\n",
    "print('Array shape after (valid, train):')\n",
    "print(sirenNoiseVal.shape)\n",
    "print(sirenNoise.shape)\n",
    "print(backNoiseVal.shape)\n",
    "print(backNoise.shape)\n",
    "print(carNoiseVal.shape)\n",
    "print(carNoise.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "\n",
    "sos = signal.butter(5, [50, 5000], 'bandpass', fs=RATE, output='sos')\n",
    "\n",
    "def get_mfccs(audio):\n",
    "   \n",
    "    try:\n",
    "        audio = 2*((audio-min(audio))/(max(audio)-min(audio)))-1\n",
    "\n",
    "        #Filter\n",
    "        audio = signal.sosfilt(sos, audio)\n",
    "        \n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=RATE, n_mfcc=40)\n",
    "\n",
    "    except Exception as e:\n",
    "            print(\"Error encountered for a file\")\n",
    "            return None\n",
    "\n",
    "    return mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## Fit #1\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint \n",
    "from datetime import datetime \n",
    "import math\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "# Parameters\n",
    "num_samples = 999 # number of samples used for training from each array\n",
    "num_samp_noise = math.floor(num_samples/3)*4 # need more noise samples than siren samples\n",
    "\n",
    "Db5gain = 1.778279 # gain for 5dB increase\n",
    "\n",
    "\n",
    "\n",
    "def generateData(carNoise, backNoise, sirenNoise):\n",
    "# This function \n",
    "# 1) combines siren and noise data to create 2 groups: siren present group, siren not present group\n",
    "# siren group: siren + car, siren + environment, siren + car + environmental (4 signal to noise ratios for each group)\n",
    "# no siren group: car, environment, car + environmental \n",
    "# Then\n",
    "# 2) Normalizes the audio and extracts\n",
    "\n",
    "\n",
    "    start = datetime.now()\n",
    "\n",
    "    # get desired number of samples from each array\n",
    "    if len(carNoise) < num_samp_noise:\n",
    "        extra = num_samp_noise - len(carNoise)\n",
    "        perm = np.random.permutation(len(carNoise))\n",
    "        carX = np.concatenate((carNoise[perm], carNoise[perm[0:extra]]))\n",
    "        if len(carX) < num_samp_noise:\n",
    "            extra = num_samp_noise - len(carX)\n",
    "            perm = np.random.permutation(len(carX))\n",
    "            carX = np.concatenate((carX[perm], carX[perm[0:extra]]))\n",
    "    else:\n",
    "        perm = np.random.permutation(len(carNoise))[0:num_samp_noise]\n",
    "        carX = carNoise[perm]\n",
    "\n",
    "    if len(backNoise) < num_samp_noise:\n",
    "        extra = num_samp_noise - len(backNoise)\n",
    "        perm = np.random.permutation(len(backNoise))\n",
    "        envX = np.concatenate((backNoise[perm], backNoise[perm[0:extra]]))\n",
    "    else:\n",
    "        perm = np.random.permutation(len(backNoise))[0:num_samp_noise]\n",
    "        envX = backNoise[perm]\n",
    "\n",
    "    if len(sirenNoise) < num_samples:\n",
    "        extra = num_samples - len(sirenNoise)\n",
    "        perm = np.random.permutation(len(sirenNoise))\n",
    "        sirenX = np.concatenate((sirenNoise[perm], sirenNoise[perm[0:extra]]))\n",
    "    else:\n",
    "        perm = np.random.permutation(len(sirenNoise))[0:num_samples]\n",
    "        sirenX = sirenNoise[perm]\n",
    "\n",
    "\n",
    "    # Add gain to background noise\n",
    "    splitback1 = math.floor(num_samples/3)\n",
    "\n",
    "    carX[0:splitback1] = carX[0:splitback1]*(1/(2*Db5gain))\n",
    "    carX[splitback1:splitback1*2] = carX[splitback1:splitback1*2]*(1/Db5gain)\n",
    "    carX[splitback1*2:splitback1*3] = carX[splitback1*2:splitback1*3]\n",
    "    carX[splitback1*3:] = carX[splitback1*3:]*Db5gain\n",
    "\n",
    "    envX[0:splitback1] = envX[0:splitback1]*(1/(2*Db5gain))\n",
    "    envX[splitback1:splitback1*2] = envX[splitback1:splitback1*2]*(1/Db5gain)\n",
    "    envX[splitback1*2:splitback1*3] = envX[splitback1*2:splitback1*3]\n",
    "    envX[splitback1*3:] = envX[splitback1*3:]*Db5gain\n",
    "\n",
    "    #sirenX[0:splitback1] = sirenX[0:splitback1]\n",
    "    #sirenX[splitback1:splitback1*2] = envX[splitback1:splitback1*2]*Db5gain\n",
    "    #sirenX[splitback1*2:splitback1*3] = envX[splitback1*2:splitback1*3]*Db5gain*2\n",
    "    #sirenX[splitback1*3:] = sirenX[splitback1*3:]*Db5gain*3\n",
    "\n",
    "    sirencar = sirenX[0:splitback1]+carX[0:splitback1]\n",
    "    sirenenv = sirenX[splitback1:splitback1*2]+envX[0:splitback1]\n",
    "    sirencarenv = sirenX[splitback1*2:splitback1*3]+carX[splitback1:splitback1*2]+envX[splitback1:splitback1*2]\n",
    "\n",
    "    sirenSet = np.concatenate((sirencar, sirenenv, sirencarenv))\n",
    "    noiseSet = np.concatenate((carX[splitback1*2:splitback1*3], envX[splitback1*2:splitback1*3], carX[splitback1*3:splitback1*4]+envX[splitback1*3:splitback1*4]))            \n",
    "\n",
    "    print('Finished scrambling data set')\n",
    "    print('Extracting MFCCs...')\n",
    "\n",
    "    mfccSiren = []\n",
    "    for i in range(0,len(sirenSet)):\n",
    "        cur_audio = sirenSet[i]\n",
    "        #cur_audio = 2*((cur_audio-min(cur_audio))/(max(cur_audio)-min(cur_audio)))-1\n",
    "        mfcc = get_mfccs(cur_audio)\n",
    "        mfccSiren.append(mfcc)\n",
    "        sirenSet[i] = cur_audio\n",
    "\n",
    "    mfccNoise = []\n",
    "    for i in range(0,len(noiseSet)):\n",
    "        cur_audio = noiseSet[i]\n",
    "        #cur_audio = 2*((cur_audio-min(cur_audio))/(max(cur_audio)-min(cur_audio)))-1\n",
    "        mfcc = get_mfccs(cur_audio)\n",
    "        mfccNoise.append(mfcc)\n",
    "        noiseSet[i] = cur_audio\n",
    "    \n",
    "    # Create labels for the data\n",
    "    sirenLabels = ['siren']*len(mfccSiren) \n",
    "    noiseLabels = ['noise']*len(mfccNoise)\n",
    "\n",
    "    # Comibine MFCCs and labels \n",
    "    featureSiren = list(zip(mfccSiren, sirenLabels))\n",
    "    featureNoise = list(zip(mfccNoise, noiseLabels))\n",
    "    features = featureSiren + featureNoise\n",
    "    random.shuffle(features)\n",
    "    \n",
    "    # Convert to Panda dataframe\n",
    "    featuresdf = pd.DataFrame(features, columns=['feature','class_label'])\n",
    "\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from keras.utils import to_categorical\n",
    "\n",
    "    # Convert features and corresponding classification labels into numpy arrays\n",
    "    X = np.array(featuresdf.feature.tolist())\n",
    "    y = np.array(featuresdf.class_label.tolist())\n",
    "\n",
    "    # Encode the classification labels\n",
    "    le = LabelEncoder()\n",
    "    yy = to_categorical(le.fit_transform(y)) \n",
    "\n",
    "    # split the dataset \n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, yy, test_size=0.2)#, random_state = 42)\n",
    "\n",
    "    num_rows = 40\n",
    "    num_columns = 130\n",
    "    num_channels = 1\n",
    "\n",
    "    x_train = x_train.reshape(x_train.shape[0], num_rows, num_columns, num_channels)\n",
    "    x_test = x_test.reshape(x_test.shape[0], num_rows, num_columns, num_channels)\n",
    "\n",
    "    duration = datetime.now() - start\n",
    "    print(\"Generating training and test data took: \", duration)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model architecture\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics \n",
    "\n",
    "num_rows = 40\n",
    "num_columns = x_train.shape[2]\n",
    "num_channels = 1\n",
    "\n",
    "num_labels = 2 #yy.shape[1]\n",
    "filter_size = 2\n",
    "\n",
    "# Construct model \n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16, kernel_size=filter_size, input_shape=(num_rows, num_columns, num_channels), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=filter_size, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=filter_size, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=filter_size, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(GlobalAveragePooling2D())\n",
    "\n",
    "model.add(Dense(num_labels, activation='softmax')) \n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer='adam') \n",
    "\n",
    "\n",
    "# Summary of model\n",
    "\n",
    "# Display model architecture summary \n",
    "model.summary()\n",
    "\n",
    "# Calculate pre-training accuracy \n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "accuracy = 100*score[1]\n",
    "\n",
    "print(\"Pre-training accuracy: %.4f%%\" % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model by regenerating training data based on num_fits\n",
    "# Each iteration of num_fits, a new combination of siren and noise is generated from the array\n",
    "# adn the model weights are fit with epochs = num_epochs and batch size = num_batch_size\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint \n",
    "from datetime import datetime \n",
    "\n",
    "#num_epochs = 12\n",
    "#num_batch_size = 128\n",
    "\n",
    "num_fits = 4 # number of times new training data is generated and model trained\n",
    "\n",
    "# For each fit (i.e. for iterations = num_fits):\n",
    "num_epochs = 25\n",
    "num_batch_size = 32\n",
    "\n",
    "saveModel = 'C:/Users/Gabe/Documents/SeniorDesign/CNNModels/weights.best.basic_cnn.hdf5'\n",
    "checkpointer = ModelCheckpoint(filepath=saveModel, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "for i in range(0, num_fits):\n",
    "    \n",
    "    print('Generating data for fit #', i+1)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = generateData(carNoise, backNoise, sirenNoise)\n",
    "    \n",
    "    start = datetime.now()\n",
    "\n",
    "    model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(x_test, y_test), callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "    duration = datetime.now() - start\n",
    "    print(\"Training completed in time: \", duration)\n",
    "\n",
    "    # Evaluating the model on the training and testing set\n",
    "    \n",
    "    print('Results from fit #', i+1)\n",
    "    score = model.evaluate(x_train, y_train, verbose=0)\n",
    "    print(\"Training Accuracy: \", score[1])\n",
    "\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print(\"Testing Accuracy: \", score[1])\n",
    "    \n",
    "print('Finished model fitting')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
